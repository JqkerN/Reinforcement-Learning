 \documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{amsmath}
\usepackage[labelsep=period]{caption} % Figur. istället för Figur:
\usepackage{relsize}
\usepackage{multirow}
\usepackage[export]{adjustbox}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{array}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{parskip}
\newcommand\tab[1][1cm]{\hspace*{#1}}

\title{Reinforcement Learning EL2805\\
       Laboration 2}

\author{Ilian Corenliussen, 950418-2438, ilianc@kth.se\\ 
        Daniel Hirsch, 960202-5737, dhirsch@kth.se}
\date{December 2020}

\begin{document}

\maketitle

\section*{Problem 1: \\
        Deep Q-Networks (DQN)}
\subsection*{A)}

\subsection*{B)} In a DQN a replay buffer is used to stabilize the training process. This is done before the update of the neural network, by decorrelation of the training samples for each batch. 

\subsection*{C)}

\subsection*{D)}
We chose to implement a network with one hidden layer, the input layer uses $8$ neurons to match the 8-dimensional state of the problem. The hidden layer had the size of $50$ neurons. The optimizer chosen was the Adam optimizer as it was suggested for these types of problems. The clipping value was set to $1$.\\% after performing tests of the clipping value between $0.5 - 2$. 

\noindent The size of the experience replay buffer parameter $L$ was chosen to $20000$ after iterating through values between $5000 - 30000$. The parameter $N$, i.e. the size of the training batch were chosen to $24$ to match the complexity of our problem, as it generally is in the order of $4 - 128$.
The update frequency of the target network, $C$, were chosen to be $C = L/N$ as it is the general suggestion for the update frequency. The number of episodes $T_E$ where iterated between $100$ to $1000$, and the chosen value were $600$. The number of episodes were chosen to $600$ because no significant performance inrease where found for more episodes, also when training the neural network to long it starts forgetting. $\epsilon$ were chosen to exponentially decay between the values of $0.99$ and $0.05$. The discount factor $\gamma$ was set to $0.99$. 

\subsection*{E)}
\subsubsection*{1)}
As can be seen in Figure \ref{fig:Policy} the total reward goes up for the training and stops to decrease after approximately 580 episodes. The number of steps does also steady decrease after 200 episodes. Where the hyperparameters where selected as described under Section D.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Lab_2/problem1/images/First_run.png}
    \caption{\small Total episodic reward and the number of steps taken per episodes during training for the final optimized policy. }
    \label{fig:Policy}
\end{figure}

\subsubsection*{2)}


\subsubsection*{3)}


\subsection*{F)}

\subsection*{G)}

\subsection*{H)}




\end{document}
