 \documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{amsmath}
\usepackage[labelsep=period]{caption} % Figur. istället för Figur:
\usepackage{relsize}
\usepackage{multirow}
\usepackage[export]{adjustbox}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{array}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{parskip}
\newcommand\tab[1][1cm]{\hspace*{#1}}

\title{Reinforcement Learning EL2805\\
       Laboration 2}

\author{Ilian Corenliussen, 950418-2438, ilianc@kth.se\\ 
        Daniel Hirsch, 960202-5737, dhirsch@kth.se}
\date{December 2020}

\begin{document}

\maketitle

\section*{Problem 1: \\
        Deep Q-Networks (DQN)}

\subsection*{B)} In a DQN a replay buffer is used to stabilize the learning process. By mixing previous with new experience to make the network train on a mixed experiences rather then only consider new experience. This is done before the update of the neural network, to decorrelation the training samples for each batch. The target network is used to have a stable target values too compare to, instead of updating after every step we update at every $C$-step and thus we have a stable copy of the network(target network) too compare to.

\subsection*{C)}
Requirements fulfilled as can be seen in Figure \ref{fig:passed_test}, with an total reward of $141.1 \pm 28.9$ with the confidence of $95 \%$.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Lab_2/problem1/images/passed_test.png}
    \caption{\small Average total reward over 50 points on average over 50 episodes - test passed. }
    \label{fig:passed_test}
\end{figure}



\subsection*{D)}
We chose to implement a network with one hidden layer, the input layer uses $8$ neurons to match the 8-dimensional state of the problem. The hidden layer had the size of $50$ neurons. The optimizer chosen was the Adam optimizer as it was suggested for these types of problems. The clipping value was set to $1$.\\
\noindent The size of the experience replay buffer parameter $L$ was chosen to $20000$ after iterating through values between $5000 - 30000$. The parameter $N$, i.e. the size of the training batch were chosen to $24$ to match the complexity of our problem, as it generally is in the order of $4 - 128$.
The update frequency of the target network, $C$, were chosen to be $C = L/N$ as it is the general suggestion for the update frequency. The number of episodes $T_E$ where iterated between $100$ to $1000$, and the chosen value were $600$. The number of episodes were chosen to $600$ because no significant performance increase where found for more episodes, also when training the neural network to long it starts forgetting. $\epsilon$ were chosen to exponentially decay between the values of $0.99$ and $0.05$. The discount factor $\gamma$ was set to $0.99$. 

\subsection*{E)}
\subsubsection*{1)}
As can be seen in Figure \ref{fig:Policy} the total reward goes up for the training and reaches a peak at 600 episodes. The number of steps does also steady decrease after 200 episodes. Where the hyperparameters where selected as described under Section D.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Lab_2/problem1/images/main_network_2.png}
    \caption{\small Total episodic reward and the number of steps taken per episodes during training for the final optimized policy. }
    \label{fig:Policy}
\end{figure}

\subsubsection*{2)}
 When using $\gamma_1$ the lander mostly took the approach of hovering, due to infinity sum of rewards from far in the future see Figure \ref{fig:gamma1}. A too low gamma, such as $\gamma_2$, the lander took a too aggressive approach of descent which resulted in a less satisfying reward score see Figure \ref{fig:gamma0.2}.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Lab_2/problem1/images/gamma_1.png}
    \caption{\small Total episodic reward and the number of steps taken per episodes during training with $\gamma_1$.  }
    \label{fig:gamma1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Lab_2/problem1/images/gamma_02.png}
    \caption{\small Total episodic reward and the number of steps taken per episodes during training with $\gamma_2$. }
    \label{fig:gamma0.2}
\end{figure}

\subsubsection*{3)}
When using 1000 episodes the average reward does not decrease, i.e. no catastrophic memory loss, but it does not have any significant improvement either see Figure \ref{fig:episodes1000}. When increasing the buffer size to $5000$, a decrease in the average reward is obtained, see Figure \ref{fig:buffer5000}. The training process also seems to be more stable, at least for the first $350$ episodes. But it fluctuates more in its behaviours due to it more frequently fills its buffer with new experiences.  
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Lab_2/problem1/images/episodes1000.png}
    \caption{\small Total episodic reward and the number of steps taken per episodes during training, trained for 1000 episodes. }
    \label{fig:episodes1000}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Lab_2/problem1/images/buffer5000.png}
    \caption{\small Total episodic reward and the number of steps taken per episodes during training, trained for 1000 episodes. With a buffer size of $5000$. }
    \label{fig:buffer5000}
\end{figure}


\subsection*{F)}
\subsubsection*{1)}
The $\max_a Q_\theta(s(y,\omega),a)$ for the state $S(y,\omega) = (0,y,0,0,\omega,0,0,0)$ with $\omega \in [-\pi,\pi] $ and $y \in [0, 1.5]$, see Figure \ref{fig:Q_value}, the highest values where proportional to the angel of the lander. Also there were a small incline depending on the height from the ground of the lander. 
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Lab_2/problem1/images/Q_values_2.png}
    \caption{\small the Q values with $\omega \in [-\pi,\pi] $ and $y \in [0, 1.5]$.}
    \label{fig:Q_value}
\end{figure}

\subsubsection*{2)}
The $\argmax_a Q_\theta(s(y,\omega),a)$ for the state $S(y,\omega) = (0,y,0,0,\omega,0,0,0)$ with $\omega \in [-\pi,\pi] $ and $y \in [0, 1.5]$, see Figure \ref{fig:Q_action}, one could notice that the lander compensates with the left and right motor depending on the angle of the lander. One interesting finding is that the lander does not activate the main motor in any of the states.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Lab_2/problem1/images/Q_action_2.png}    \caption{\small the Q actions with $\omega \in [-\pi,\pi] $ and $y \in [0, 1.5]$.}
    \label{fig:Q_action}
\end{figure}

\subsection*{G)}
If comparing Figure \ref{fig:Policy} to Figure \ref{fig:randomAgent}, we see that the total average reward over 50 episodes for the RandomAgent is approximately $-210$ while our Q-network has a total average reward of $141$. The comparison also shows that the RandomAgent does not learn and improve its reward during the whole period of episodes.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Lab_2/problem1/images/randomAgent.png}
    \caption{\small FTotal episodic reward and the number of steps taken per episodes during training for the RandomAgent.  }
    \label{fig:randomAgent}
\end{figure}




\end{document}
