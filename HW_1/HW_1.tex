 \documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{amsmath}
\usepackage[labelsep=period]{caption} % Figur. istället för Figur:
\usepackage{relsize}
\usepackage{multirow}
\usepackage[export]{adjustbox}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{array}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{parskip}

\title{Reinforcement Learning\\
       Homework 1}

\author{Ilian Corenliussen, 950418-2438, ilianc@kth.se\\ 
        Daniel Hirsch, 960202-5737, dhirsch@kth.se}
\date{November 2020}

\begin{document}

\maketitle

\section{Machine Replacement}
\subsection*{a)}
\textbf{State space:} $S = \{S_P(perfect), S_W(worn), S_B(broken)\} $\\
\textbf{Actions}: $A = \{ A_C (continue), A_R(replace) \}$\\
\textbf{Rewards:} $r(S_P,A_C) = 0$, $r(S_W,A_C) = -\frac{c}{2}$, $r(S_B,A_C) = -c$, $r(S_P, A_R) = -R$, $r(S_W, A_R) = -R$, $r(S_B, A_R) = -R$


The transition probability for replace should always be 1 to go from any state to perfect state, i.e. $p(S_P|S_i, A_R) = 1$ for all $S_i \in S$.  
\[
		P(replace) = \begin{bmatrix}
		        1 & 0 & 0 \\
		        1 & 0 & 0\\
		        1 & 0 & 0
		        \end{bmatrix}
\]
The probability to go from one state to a more worn state when continuing is modeled with the probability $\theta$, i.e. $p(S_W|S_P,A_C)=\theta, p(S_B|S_W,A_C)=\theta$. The probability to stay in the current state will then be $p(S_P|S_P,A_C)=p(S_W|S_W,A_C)=1-\theta$ and the probability to stay in broken when the current state is broken should be equal to 1.
\[
		P(continue) = \begin{bmatrix}
		        1-\theta & \theta & 0 \\
		        0 & 1-\theta & \theta\\
		        0 & 0 & 1
		        \end{bmatrix}
\]
\subsection*{b)}
\textbf{Bellman equation:} $u_{t}^{\star}\left(s_{t}\right)=\max _{a \in A_{s_{t}}}\left[r_{t}\left(s_{t}, a\right)+\sum_{j \in S} p_{t}\left(j \mid s_{t}, a\right) u_{t+1}^{\star}\left(s_{t}, a, j\right)\right]$

\subsubsection*{Solution:}
\begin{equation}
u_{2}^{*}(S_i \in S)  = 0
\end{equation}

\begin{equation}
    u_{1}^{*}(S_P) = \max
    \left\{\begin{matrix}
    A_C: & 0 \\
    A_R: & -R = -8
    \end{matrix}\right\}
    = 0
\end{equation}

\begin{equation}
    u_{1}^{*}(S_W) = \max
    \left\{\begin{matrix}
    A_C: & -\frac{c}{2} = -3 \\
    A_R: & -R = -8
    \end{matrix}\right\}
    = -3
\end{equation}


\begin{equation}
    u_{1}^{*}(S_B) = \max
    \left\{\begin{matrix}
    A_C: & -c = -6\\
    A_R: & -R = -8
    \end{matrix}\right\}
    = -6
\end{equation}


\begin{equation}
    u_{0}^{*}(S_P) = \max
    \left\{\begin{matrix}
    A_C: & 0 + \Theta u_{1}^{*}(W) + (1 - \Theta)u_{1}^{*}(P) = -1.5 \\
    A_R: & -R + u_{1}^{*}(P) = -8
    \end{matrix}\right\}
    = -1.5
\end{equation}

\begin{equation}
    u_{0}^{*}(S_W) = \max
    \left\{\begin{matrix}
    A_C: & -\frac{c}{2} + \Theta u_{1}^{*}(B) + (1 - \Theta)u_{1}^{*}(W) = -7.5\\
    A_R: & -R +  u_{1}^{*}(P) = -8
    \end{matrix}\right\}
    = -7.5
\end{equation}

\begin{equation}
    u_{0}^{*}(S_B) = \max
    \left\{\begin{matrix}
    A_C: & -c + u_{1}^{*}(B) = -12\\
    A_R: & -R + u_{1}^{*}(P) = -8
    \end{matrix}\right\}
    = -8
\end{equation}






\subsubsection*{Answer:}
$$u^*_0(Worn)= -7.5$$
$$a^*_0(Broken)=Replace$$


\section{Optimal Stopping}
\subsection*{a)}
\subsubsection*{Motivation:}
The number of possible states for the MDP are equal to the number of tosses, where each toss can generate a new head, i.e. $S = \{(0,0),(n, t) \; \text{for} \; n,t\in T\}$. The probability to move to a state with a higher number of heads than current state will be equal to $0.5$ due to fair coin. Furthermore, the probabilty to stay in the current state will be equal to $0.5$ as well. The reward for stopping will be proportional to the number of head ($n$) divided by the number of tosses ($t$), i.e. calculate the final score as $r(S_i, A_S) = \frac{n}{t} \; \forall \; S_i \in S$. The reward for continue will be equal to zero, $r(S_i, A_C) = 0 \; \forall \; S_i \in S $ due to we will not receive any reward for continuing tossing the coin.

The number of states will be equal to an under triangular matrix such as, 
\[
A_{Tx T} = \begin{bmatrix} 
    (n=0,t=0) &  & n/s \\
    \vdots & \ddots & \\
    (n=0, t=T) & \dots&(n=T,t=T) 
    \end{bmatrix}
\]
Where  $n/s=\text{not a state}$.

\textbf{Summarized}\\
\textbf{States:} $S = \{(0,0),(n, t) \; \text{for} \; n,t\in T\}$ \\
\textbf{Actions:} $A = \{A_C(continue), A_S(stop)\}$ \\
\textbf{Rewards:}  $r(S_i, A_C) = 0$ and $r(S_i, A_S) = \frac{n}{t}$ $ \; \forall \; S_i \in S$  \\







\subsubsection*{Answer:}
\textit{Number of states:} will be equal to the number of elements in an under triangular matrix with the size TxT, i.e. $\frac{1}{2}T(T+1)$ number of states.

\subsection*{b)}
\subsection*{c)}
\subsection*{d)}

\end{document}
